{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMjC0sHmWToR3JCzhNvA1OP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carissa406/CSC533/blob/main/Topic_Modeling_COVID_tweets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDQQHwomQV7N",
        "outputId": "e1d41cdf-e451-40a7-f5d4-70c9ef301e8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"1.8.0_312\"\n",
            "OpenJDK Runtime Environment (build 1.8.0_312-8u312-b07-0ubuntu1~18.04-b07)\n",
            "OpenJDK 64-Bit Server VM (build 25.312-b07, mixed mode)\n",
            "Collecting pyspark==2.4.4\n",
            "  Downloading pyspark-2.4.4.tar.gz (215.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 215.7 MB 58 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.7\n",
            "  Downloading py4j-0.10.7-py2.py3-none-any.whl (197 kB)\n",
            "\u001b[K     |████████████████████████████████| 197 kB 19.7 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-2.4.4-py2.py3-none-any.whl size=216130392 sha256=18a7c6206f0598ba875d48f94f78ba451f0a10dbaca0965ad9e88ac9f651d179\n",
            "  Stored in directory: /root/.cache/pip/wheels/11/48/19/c3b6b66e4575c164407a83bc065179904ddc33c9d6500846f0\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.7 pyspark-2.4.4\n",
            "Collecting spark-nlp==2.6.3\n",
            "  Downloading spark_nlp-2.6.3-py2.py3-none-any.whl (129 kB)\n",
            "\u001b[K     |████████████████████████████████| 129 kB 5.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: spark-nlp\n",
            "Successfully installed spark-nlp-2.6.3\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "# Install java\n",
        "! apt-get update -qq\n",
        "! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
        "! java -version\n",
        "# Install pyspark\n",
        "! pip install --ignore-installed pyspark==2.4.4\n",
        "# Install Spark NLP\n",
        "! pip install --ignore-installed spark-nlp==2.6.3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml import Pipeline\n",
        "from sparknlp.base import *\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp.pretrained import PretrainedPipeline\n",
        "import sparknlp\n",
        "spark = sparknlp.start()\n",
        "print(\"Spark NLP version: \", sparknlp.version())\n",
        "print(\"Apache Spark version: \", spark.version)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwUskZJQRD0l",
        "outputId": "366c9049-2e25-470d-e207-27afb017ea21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark NLP version:  2.6.3\n",
            "Apache Spark version:  2.4.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # if you are reading file from local storage\n",
        "file_location = '/content/coronavirus-text-only-1000.txt'\n",
        "file_type = \"csv\"\n",
        "# CSV options\n",
        "infer_schema = \"true\"\n",
        "first_row_is_header = \"true\"\n",
        "delimiter = \",\"\n",
        "df = spark.read.format(file_type) \\\n",
        " .option(\"inferSchema\", infer_schema) \\\n",
        " .option(\"header\", first_row_is_header) \\\n",
        " .option(\"sep\", delimiter) \\\n",
        " .load(file_location)\n",
        "# Verify the count\n",
        "df.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-eQQ2agRXwY",
        "outputId": "22d10ffb-d0e9-4ba3-e154-b882e1acde19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "999"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(20, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MCJ0u3KSMwk",
        "outputId": "20783e99-2c5d-406b-9990-39d482088c2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|text                                                                                                                                                 |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|Studies look at the potential of natural remedies for treating coronavirus https://t.co/UQMDXZCDTE                                                   |\n",
            "|RT @EricTopol: These rapid home tests, especially if accurate for transmissibility and cheap, can be transformative and are likely to hit t‚Ä¶       |\n",
            "|RT @NPR: Working moms now spend 15 more hours than working dads on childcare and housework, a recent survey finds. But fewer work hours is‚Ä¶        |\n",
            "|\"RT @Harvey_Walker96: To Al Jazeera,   Malaysia didnt lock these Illegal Immigrants because of \"\"racism\"\" and all that. It is because they ill‚Ä¶\"   |\n",
            "|RT @CNNEE: La farmac√©utica estadounidense Pfizer y la compa√±√≠a alemana de biotecnolog√≠a BioNTech informaron que las primeras pruebas de una‚Ä¶   |\n",
            "|RT @ReutersWorld: Hundreds of scientists say coronavirus is airborne, ask WHO to revise recommendations: NYT https://t.co/ARz04pzJt5 https:‚Ä¶       |\n",
            "|RT @CNN: This Illinois teen's coronavirus-themed dress, made entirely out of duct tape, features multiple images depicting life during the‚Ä¶        |\n",
            "|\"RT @Censelio: Argentina: Organizaba marchas anticuarentena y muri√≥ por coronavirus | \"\"Dec√≠a que los comunistas no ten√≠an que volver\"\" https:‚Ä¶\"|\n",
            "|RT @jilevin: Trump Falsely Claims '99 Percent' of Virus Cases Are 'Totally Harmless' https://t.co/DdlYJ4IpcL                                         |\n",
            "|RT @propublica: President Donald Trump and Vice President Mike Pence have repeatedly attributed the increase in the coronavirus case count‚Ä¶        |\n",
            "|NSW to close Victorian border after COVID crisis talks https://t.co/VchwTMgRZN                                                                       |\n",
            "|RT @ASlavitt: Trump‚Äôs new Coronavirus ‚Äújust live with it‚Äù message should play well with voters over 65.                                        |\n",
            "|RT @ClayTravis: Coronavirus deaths dropped to 209 today, a new low in the country since March 23rd. (Yesterday there were just 254 deaths n‚Ä¶       |\n",
            "|RT @JamesGunn: I've known a few people who have had coronavirus. The ones over 65 all died. 2 in their 30's &amp; 1 in her 50's have been sick‚Ä¶    |\n",
            "|RT @NatashaFatah: 239 scientists in 32 countries are challenging the W-H-O's COVID-19 recommendations and say there is evidence that the vi‚Ä¶       |\n",
            "|RT @crissles: Y‚Äôall really decided that being at the club, on a boat, on the beach etc mattered more than controlling the spread of coronav‚Ä¶     |\n",
            "|\"RT @Censelio: Argentina: Organizaba marchas anticuarentena y muri√≥ por coronavirus | \"\"Dec√≠a que los comunistas no ten√≠an que volver\"\" https:‚Ä¶\"|\n",
            "|RT @Villarruel_clau: Organizaba marchas anti cuarentena y muri√≥ por coronavirus: ‚ÄúMi primo dec√≠a que nadie lo iba a frenar‚Äù https://t.co/bj‚Ä¶ |\n",
            "|RT @JaxAlemany: ‚Äú...almost 40 percent of Americans earning less than $40,000 a year lost their jobs in March.‚Äù https://t.co/c9JWy8lnEf           |\n",
            "|RT @JamesGunn: I've known a few people who have had coronavirus. The ones over 65 all died. 2 in their 30's &amp; 1 in her 50's have been sick‚Ä¶    |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#convert to document\n",
        "document_assembler = DocumentAssembler() \\\n",
        " .setInputCol(\"text\") \\\n",
        " .setOutputCol(\"document\") \\\n",
        " .setCleanupMode(\"shrink\")"
      ],
      "metadata": {
        "id": "jEFegq3GS7PV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#split sentence to tokens(array)\n",
        "tokenizer = Tokenizer() \\\n",
        " .setInputCols([\"document\"]) \\\n",
        " .setOutputCol(\"token\")"
      ],
      "metadata": {
        "id": "BT0zrVRuTeRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #normalizing\n",
        " normalizer = Normalizer() \\\n",
        " .setInputCols([\"token\"]) \\\n",
        " .setOutputCol(\"normalized\")"
      ],
      "metadata": {
        "id": "KfBc8-ygTgtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #stopword removal + removing coronavirus as a stopword\n",
        " stopwords_cleaner = StopWordsCleaner()\\\n",
        " .setInputCols(\"normalized\")\\\n",
        " .setOutputCol(\"cleanTokens\")\\\n",
        " .setStopWords([\"coronavirus\"])\\\n",
        " .setCaseSensitive(False)"
      ],
      "metadata": {
        "id": "Fr59U4g9TiOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #stemming\n",
        " stemmer = Stemmer() \\\n",
        " .setInputCols([\"cleanTokens\"]) \\\n",
        " .setOutputCol(\"stem\")"
      ],
      "metadata": {
        "id": "51BAbTQsTj3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #finishing back to array of tokens\n",
        " finisher = Finisher() \\\n",
        " .setInputCols([\"stem\"]) \\\n",
        " .setOutputCols([\"tokens\"]) \\\n",
        " .setOutputAsArray(True) \\\n",
        " .setCleanAnnotations(False)"
      ],
      "metadata": {
        "id": "2uZrvLJ8Tlv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #build ML pipeline\n",
        " nlp_pipeline = Pipeline(\n",
        " stages=[document_assembler,\n",
        " tokenizer,\n",
        " normalizer,\n",
        " stopwords_cleaner,\n",
        " stemmer,\n",
        " finisher])"
      ],
      "metadata": {
        "id": "rd2ZN8d0To2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #train and apply the ML pipeline\n",
        " nlp_model = nlp_pipeline.fit(df)\n",
        "processed_df = nlp_model.transform(df)\n",
        "tokens_df = processed_df.select('tokens').limit(10000)\n",
        "tokens_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7odqo_cTpwY",
        "outputId": "88d4598a-f6c7-4aff-c4fe-6eb11effe416"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|              tokens|\n",
            "+--------------------+\n",
            "|[studi, look, at,...|\n",
            "|[rt, erictopol, t...|\n",
            "|[rt, npr, work, m...|\n",
            "|[rt, harveywalk, ...|\n",
            "|[rt, cnnee, la, f...|\n",
            "|[rt, reutersworld...|\n",
            "|[rt, cnn, thi, il...|\n",
            "|[rt, censelio, ar...|\n",
            "|[rt, jilevin, tru...|\n",
            "|[rt, propublica, ...|\n",
            "|[nsw, to, close, ...|\n",
            "|[rt, aslavitt, tr...|\n",
            "|[rt, claytravi, d...|\n",
            "|[rt, jamesgunn, i...|\n",
            "|[rt, natashafatah...|\n",
            "|[rt, crissl, yäôa...|\n",
            "|[rt, censelio, ar...|\n",
            "|[rt, villarruelcl...|\n",
            "|[rt, jaxalemani, ...|\n",
            "|[rt, jamesgunn, i...|\n",
            "+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aeA5L5eVhJL",
        "outputId": "7b18c23c-11e9-411c-d16f-32143a6572e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|tokens                                                                                                                                                 |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|[studi, look, at, the, potenti, of, natur, remedi, for, treat, httpstcouqmdxzcdt]                                                                      |\n",
            "|[rt, erictopol, these, rapid, home, test, especi, if, accur, for, transmiss, and, cheap, can, be, transform, and, ar, like, to, hit, tä]               |\n",
            "|[rt, npr, work, mom, now, spend, more, hour, than, work, dad, on, childcar, and, housework, a, recent, survei, find, but, fewer, work, hour, isä]      |\n",
            "|[rt, harveywalk, to, al, jazeera, malaysia, didnt, lock, these, illeg, immigr, becaus, of, racism, and, all, that, it, i, becaus, thei, illä]          |\n",
            "|[rt, cnnee, la, farmacutica, estadounidens, pfizer, y, la, compaa, alemana, de, biotecnologa, biontech, informaron, que, la, primera, prueba, de, unaä]|\n",
            "|[rt, reutersworld, hundr, of, scientist, sai, i, airborn, ask, who, to, revis, recommend, nyt, httpstcoarzpzjt, httpsä]                                |\n",
            "|[rt, cnn, thi, illinoi, teen, coronavirusthem, dress, made, entir, out, of, duct, tape, featur, multipl, imag, depict, life, dure, theä]               |\n",
            "|[rt, censelio, argentina, organizaba, marcha, anticuarentena, y, muri, por, deca, que, lo, comunista, no, tenan, que, volver, httpsä]                  |\n",
            "|[rt, jilevin, trump, fals, claim, percent, of, viru, case, ar, total, harmless, httpstcoddlyjipcl]                                                     |\n",
            "|[rt, propublica, presid, donald, trump, and, vice, presid, mike, penc, have, repeatedli, attribut, the, increas, in, the, case, countä]                |\n",
            "|[nsw, to, close, victorian, border, after, covid, crisi, talk, httpstcovchwtmgrzn]                                                                     |\n",
            "|[rt, aslavitt, trumpäô, new, äújust, live, with, itäù, messag, should, plai, well, with, voter, over]                                                  |\n",
            "|[rt, claytravi, death, drop, to, todai, a, new, low, in, the, countri, sinc, march, rd, yesterdai, there, were, just, death, nä]                       |\n",
            "|[rt, jamesgunn, iv, known, a, few, peopl, who, have, had, the, on, over, all, di, in, their, , amp, in, her, , have, been, sickä]                      |\n",
            "|[rt, natashafatah, scientist, in, countri, ar, challeng, the, who, covid, recommend, and, sai, there, i, evid, that, the, viä]                         |\n",
            "|[rt, crissl, yäôall, realli, decid, that, be, at, the, club, on, a, boat, on, the, beach, etc, matter, more, than, control, the, spread, of, coronavä] |\n",
            "|[rt, censelio, argentina, organizaba, marcha, anticuarentena, y, muri, por, deca, que, lo, comunista, no, tenan, que, volver, httpsä]                  |\n",
            "|[rt, villarruelclau, organizaba, marcha, anti, cuarentena, y, muri, por, äúmi, primo, deca, que, nadi, lo, iba, a, frenaräù, httpstcobjä]              |\n",
            "|[rt, jaxalemani, äúalmost, percent, of, american, earn, less, than, a, year, lost, their, job, in, marchäù, httpstcocjwylnef]                          |\n",
            "|[rt, jamesgunn, iv, known, a, few, peopl, who, have, had, the, on, over, all, di, in, their, , amp, in, her, , have, been, sickä]                      |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#countvectorizer to generate features from text data\n",
        "from pyspark.ml.feature import CountVectorizer\n",
        "cv = CountVectorizer(inputCol=\"tokens\", outputCol=\"features\",\n",
        "vocabSize=500, minDF=3.0)\n",
        "# train the model\n",
        "cv_model = cv.fit(tokens_df)\n",
        "# transform the data. Output column name will be features.\n",
        "vectorized_tokens = cv_model.transform(tokens_df)"
      ],
      "metadata": {
        "id": "UPtk-97CUQwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build Latent Dirichlet Allocation model\n",
        "from pyspark.ml.clustering import LDA\n",
        "num_topics = 3\n",
        "lda = LDA(k=num_topics, maxIter=10)\n",
        "model = lda.fit(vectorized_tokens)\n",
        "ll = model.logLikelihood(vectorized_tokens)\n",
        "lp = model.logPerplexity(vectorized_tokens)\n",
        "print(\"The lower bound on the log likelihood of the entire corpus: \" +\n",
        "str(ll))\n",
        "print(\"The upper bound on perplexity: \" + str(lp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IlIYOpaYlhU",
        "outputId": "1ec2b5d3-5a24-41cf-ab07-fd1e1fb2f2da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The lower bound on the log likelihood of the entire corpus: -70365.88841696964\n",
            "The upper bound on perplexity: 5.293056146906096\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#visualize the topics\n",
        "# extract vocabulary from CountVectorizer\n",
        "vocab = cv_model.vocabulary\n",
        "topics = model.describeTopics()\n",
        "topics_rdd = topics.rdd\n",
        "topics_words = topics_rdd\\\n",
        " .map(lambda row: row['termIndices'])\\\n",
        " .map(lambda idx_list: [vocab[idx] for idx in idx_list])\\\n",
        " .collect()\n",
        "for idx, topic in enumerate(topics_words):\n",
        " print(\"topic: {}\".format(idx))\n",
        " print(\"*\"*25)\n",
        " for word in topic:\n",
        "  print(word)\n",
        " print(\"*\"*25)\n",
        " "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWW3jXyZYoYm",
        "outputId": "abe60de4-c969-4df1-c135-a7a2baa3ffbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "topic: 0\n",
            "*************************\n",
            "the\n",
            "rt\n",
            "a\n",
            "of\n",
            "and\n",
            "to\n",
            "in\n",
            "i\n",
            "that\n",
            "on\n",
            "*************************\n",
            "topic: 1\n",
            "*************************\n",
            "de\n",
            "rt\n",
            "la\n",
            "en\n",
            "el\n",
            "a\n",
            "que\n",
            "y\n",
            "por\n",
            "lo\n",
            "*************************\n",
            "topic: 2\n",
            "*************************\n",
            "rt\n",
            "the\n",
            "a\n",
            "in\n",
            "she\n",
            "trump\n",
            "to\n",
            "than\n",
            "\n",
            "job\n",
            "*************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.clustering import LDA\n",
        "num_topics = 10\n",
        "lda = LDA(k=num_topics, maxIter=100)\n",
        "model = lda.fit(vectorized_tokens)\n",
        "ll = model.logLikelihood(vectorized_tokens)\n",
        "lp = model.logPerplexity(vectorized_tokens)\n",
        "print(\"The lower bound on the log likelihood of the entire corpus: \" +\n",
        "str(ll))\n",
        "print(\"The upper bound on perplexity: \" + str(lp))\n",
        "\n",
        "vocab = cv_model.vocabulary\n",
        "topics = model.describeTopics()\n",
        "topics_rdd = topics.rdd\n",
        "topics_words = topics_rdd\\\n",
        " .map(lambda row: row['termIndices'])\\\n",
        " .map(lambda idx_list: [vocab[idx] for idx in idx_list])\\\n",
        " .collect()\n",
        "for idx, topic in enumerate(topics_words):\n",
        "  print(\"topic: {}\".format(idx))\n",
        "  print(\"*\"*25)\n",
        "  for word in topic:\n",
        "    print(word)\n",
        "  print(\"*\"*25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdyWkyRsZSBk",
        "outputId": "bd10fe58-d381-42fa-b789-2c7f62baebc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The lower bound on the log likelihood of the entire corpus: -65934.66507330502\n",
            "The upper bound on perplexity: 4.959731087205132\n",
            "topic: 0\n",
            "*************************\n",
            "work\n",
            "hour\n",
            "now\n",
            "isä\n",
            "on\n",
            "recent\n",
            "but\n",
            "more\n",
            "rt\n",
            "find\n",
            "*************************\n",
            "topic: 1\n",
            "*************************\n",
            "semana\n",
            "la\n",
            "o\n",
            "rt\n",
            "por\n",
            "aä\n",
            "e\n",
            "est\n",
            "una\n",
            "el\n",
            "*************************\n",
            "topic: 2\n",
            "*************************\n",
            "she\n",
            "said\n",
            "could\n",
            "even\n",
            "job\n",
            "chines\n",
            "rt\n",
            "donald\n",
            "trump\n",
            "the\n",
            "*************************\n",
            "topic: 3\n",
            "*************************\n",
            "de\n",
            "rt\n",
            "la\n",
            "en\n",
            "el\n",
            "que\n",
            "a\n",
            "por\n",
            "y\n",
            "lo\n",
            "*************************\n",
            "topic: 4\n",
            "*************************\n",
            "rt\n",
            "in\n",
            "th\n",
            "mai\n",
            "the\n",
            "to\n",
            "plant\n",
            "death\n",
            "trump\n",
            "april\n",
            "*************************\n",
            "topic: 5\n",
            "*************************\n",
            "mai\n",
            "todo\n",
            "made\n",
            "care\n",
            "ha\n",
            "their\n",
            "refus\n",
            "hydroxychloroquin\n",
            "fda\n",
            "should\n",
            "*************************\n",
            "topic: 6\n",
            "*************************\n",
            "florida\n",
            "sai\n",
            "rt\n",
            "msnbc\n",
            "spent\n",
            "servic\n",
            "ha\n",
            "gov\n",
            "of\n",
            "blame\n",
            "*************************\n",
            "topic: 7\n",
            "*************************\n",
            "the\n",
            "rt\n",
            "in\n",
            "a\n",
            "to\n",
            "of\n",
            "i\n",
            "have\n",
            "and\n",
            "ar\n",
            "*************************\n",
            "topic: 8\n",
            "*************************\n",
            "the\n",
            "i\n",
            "rt\n",
            "report\n",
            "white\n",
            "china\n",
            "wear\n",
            "see\n",
            "parti\n",
            "frame\n",
            "*************************\n",
            "topic: 9\n",
            "*************************\n",
            "na\n",
            "risk\n",
            "pa\n",
            "at\n",
            "todo\n",
            "que\n",
            "de\n",
            "a\n",
            "como\n",
            "rt\n",
            "*************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.clustering import LDA\n",
        "num_topics = 15\n",
        "lda = LDA(k=num_topics, maxIter=500)\n",
        "model = lda.fit(vectorized_tokens)\n",
        "ll = model.logLikelihood(vectorized_tokens)\n",
        "lp = model.logPerplexity(vectorized_tokens)\n",
        "print(\"The lower bound on the log likelihood of the entire corpus: \" +\n",
        "str(ll))\n",
        "print(\"The upper bound on perplexity: \" + str(lp))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3yStOhdaZCK",
        "outputId": "1f4de14c-43ba-4e8d-d683-c63c54ac8a3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The lower bound on the log likelihood of the entire corpus: -65097.44071699343\n",
            "The upper bound on perplexity: 4.896753476530272\n"
          ]
        }
      ]
    }
  ]
}